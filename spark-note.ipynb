{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8d17212-3303-4c3e-95f0-23b2c8f40f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from functools import reduce \n",
    "from pyspark.sql import DataFrame\n",
    "import pyspark.pandas as ps\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c40c184e-cfc0-48d4-8410-15c66c56e62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.io as pio\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from math import sqrt\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "from urllib.request import urlopen\n",
    "from os import listdir\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2a9cef1-1a50-4604-bd30-e0f8256b48be",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('folder.txt', 'r') as folder:\n",
    "    lines = folder.readlines()\n",
    "    \n",
    "folder_archivos =  ' '.join(lines)\n",
    "\n",
    "csv_files = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cb99af1-91ad-4a3f-8b92-382dc5987d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_csv_filenames( path_to_dir, suffix=\".csv\" ):\n",
    "    filenames = listdir(path_to_dir)\n",
    "    return [ filename for filename in filenames if filename.endswith( suffix ) ]\n",
    "\n",
    "csv_files = [l for l in find_csv_filenames(folder_archivos, suffix=\".csv\" ) if 'autos' in l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f61652ba-cc82-4a34-ba1c-4a805ec503f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('urls.txt', 'r') as folder_urls:\n",
    "    lines_urls = folder_urls.readlines()\n",
    "\n",
    "if len(csv_files) == 0:\n",
    "    zipurls = lines_urls\n",
    "\n",
    "    for zipurl in zipurls:\n",
    "        with urlopen(zipurl) as zipresp:\n",
    "            with ZipFile(BytesIO(zipresp.read())) as zfile:\n",
    "                zfile.extractall(folder_archivos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef3011bc-42c2-4e4c-94bf-da63d76ac3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_data_frames = []\n",
    "\n",
    "for file in csv_files:\n",
    "    df_new = spark.read.csv(file, header = True, sep = ',')\n",
    "    list_data_frames.append(df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e9c0e7f-fa4e-4194-8cf1-a7a3641c045d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unionAll(*dfs):\n",
    "    return reduce(DataFrame.unionAll, dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "29b3bbf8-77d3-4f49-8f60-9c5402bc13da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = unionAll(*list_data_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e005b98a-fa8b-48e7-9615-a38737fa6a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.groupBy(['tramite_fecha','registro_seccional_provincia']).agg(count(\"registro_seccional_provincia\").alias(\"count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d4f74dcd-a816-4617-b99e-255340cc7a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.toPandas().to_csv('df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "09cfdd73-0542-442e-bb8e-0f277dd06248",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = ps.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8c9bf071-84ef-4d7d-9c99-a89f745b2711",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.pivot(index='tramite_fecha', columns='registro_seccional_provincia', values='count').sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e8072181-26a3-4731-b9e8-547a9f1bae47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.columns.name = None              \n",
    "#df = df.reset_index() \n",
    "#df = df.rename_axis(None, axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-spark",
   "language": "python",
   "name": "env-spark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
